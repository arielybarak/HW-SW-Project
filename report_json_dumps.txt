
                        (1) Benchmark Analysis (1)

Introduction
When we write programs, we often need to save data or send it to another program.
Computers store Python objects in memory using pointers and structures that only Python understands.
To share this data, we need a common format that is simple, portable, and language-independent.
'JSON' (JavaScript Object Notation) is such a format: plain text that represents objects, arrays, numbers, strings, and a few special values.
It is easy for humans to read and for computers in any language to parse.
'CPython' is the standard Python interpreter. In CPython, the function json.dumps turns Python objects (like dictionaries and lists) into JSON text.
This process is called 'Serialization': converting in-memory structures into a portable, byte-friendly form that can be stored or transmitted.
The reverse operation, 'deserialization', turns JSON text back into Python objects. Together, these two steps let Python communicate easily with files, databases, and services written in any language.
________________________________________
Executive Summary
•	The benchmark measures how fast Python data (dicts, lists, numbers, strings) is turned into JSON, and which steps dominate time and memory.
•	Core work: visiting containers, converting values, escaping characters, formatting numbers, and building one large output string.
•	Options like pretty printing and key sorting improve readability but add significant CPU and memory cost.
•	Native implementations (Python’s built-in encoder or third-party encoders) change where time is spent.
________________________________________
Purpose & Scope
The benchmark evaluates the cost of converting Python data structures into JSON text. It focuses on speed (latency and throughput) and memory usage.
This matters because many real systems—such as web servers sending responses, applications writing logs,
or services communicating over APIs—spend a lot of time serializing data. Even small inefficiencies can multiply when millions of messages are involved.
________________________________________
 
Libraries Used
•	json (standard library): provides the dumps function.
•	pyperf: See next chapter.
•	sys: Simple command-line argument handling and exiting when needed.
________________________________________
Algorithms
•	Traversal: walk through dictionaries and lists until reaching basic values.
•	Type conversion: turn Python values (numbers, strings, booleans, None) into their JSON equivalents.
•	String building: assemble an output string with commas, braces, and brackets.
•	Escaping: replace special characters (quotes, newlines, non-English letters) with safe sequences, which makes the output larger.
•	Number formatting: convert floats into decimal text, a relatively slow step.
________________________________________
Data Structures
•	Dictionaries: collections of key–value pairs, where keys are usually strings and values can be numbers, text, booleans, None, or other containers.
•	Lists: ordered sequences of items, often used for large or repeated data.
•	Scalars: simple single values such as integers, floats, booleans, or None.
•	Strings (with Unicode): text values that may include special or non-English characters, which require extra handling during serialization.
________________________________________
Third-Party Dependencies (if any)
•	Built-in encoder: Python’s standard library uses compiled C code under the hood for speed.
•	Other encoders: third-party libraries often rely on their own compiled code (sometimes in C, sometimes in Rust). These bring in specialized number formatting or text-processing routines.
•	Runtime factors: Python’s memory management and garbage collector handle the many small pieces of data created and combined during serialization. This can affect performance in subtle ways.
________________________________________





                        (2) Initial Analysis (2)
            Performance analysis, flame graphs, profiling data


Background summary
•	pyperformance run - the harness that sets up venvs, warms up, and executes the benchmark consistently.
•	perf stat - counters (cycles, instructions) → we derive IPC and MPKI.
•	Turbo Boost - CPUs can “overclock themselves” for short bursts above base speed. Fast, but makes timings vary.
•	MSR (Model-Specific Register) - special CPU registers used by the OS/tools to control features like Turbo. Tools like pyperf read/write them.
•	CPU affinity - binding a program to a specific core so it doesn’t jump between cores (reduces noise).

•	IRQ (Interrupt Request) affinity – which CPU core handles hardware interrupts. tune moves them so they don’t disturb your benchmark core.
•	ASLR (Address Space Layout Randomization) – randomizes memory layout for security. tune reports its status since randomness can add noise.
•	pyperf system tune - helper that pins the benchmark to 1 core (affinity), tries to disable Turbo via MSR, reduces interrupt noise, and clamps sampling.
    Goal: make benchmark runs repeatable.

•	Memory churn: Lots of short-lived allocations and deallocations (creating and discarding many small temporary objects). It spends
    time in the allocator, increases pressure on CPU caches, and adds GC/reference-count overhead.
•	Dispatch: The process of selecting and executing the appropriate code handler based on some runtime condition/input, like choosing
    which case to execute based on a bytecode instruction.
    •	Polymorphic dispatch: Choosing what code to run based on the
        runtime type of an object
________________________________________
Report's findings

1. Pure Compute Bound
•	With CPI ~ 2.4 and MPKI ~ 0.02, it's definitely compute bound.

2. Interpreter execution is a major consumer
•	PyEval_EvalFrameDefault - which is the interpreter, is the single largest symbol by overhead in annotate.txt (~13%).

3. String assembly and joining are significant
•	_PyUnicode_JoinArray is the #2 entry by overhead in annotate.txt (≈6%). From that, we understand that building large output strings is a visible cost.

4. JSON C encoder’s string escaping is hot
•	py_encode_basestring_ascii is dominant(~5%) in the report, which is the string-escaping helper, used when encoding Python str objects.
•	Notice that it's a thin wrapper, so the 5% probably includes inlined/unresolved callee code – such as ascii_escape_unicode which actually does the job.

5. Type dispatch inside the encoder is non-trivial optimized
•	encoder_listencode_obj is among top entries in the report as well. 
    It is the central “router” for encoding any Python object to JSON.

6. Memory management overhead is visible
•	_PyObject_Malloc and _PyObject_Free both rank near the top table.
    From that we understand frequent small allocations/frees occur during building of the JSON output and intermediate objects.
________________________________________
Flamegraph's findings

1. Call-dispatch tower is heavy (vectorcall pipeline)
•	Deep, repetitive call chains (container walk). The encoder walks nested dicts/lists recursively, including mostly
    _PyEval_EvalFrameDefault, call_function, PyObject_Vectorcall, etc (Consistent with compute-bound).

2. User-space dominates; no heavy kernel/I/O
•	matches compute-bound: cycles spent on pure computation, not waiting on I/O or syscalls.

3. Many thin and deep leaves and no single giant leaf
•	Means many small operations, the bottleneck is cumulative per-item work.
________________________________________
* Conclusions & Suggestions *

It is trivial that the interpreter is a bottleneck. However, we can
address it not only by trying to optimize interpreter functions
directly, but also by reducing how often we rely on them. Many of the
other issues we saw: call/dispatch overhead, deep call chains, high
instruction counts (typical of compute-bound cases), and cumulative
micro-costs, stem from this reliance.

Effective solutions include adding fast paths for common types, handling
data in larger chunks instead of per-element (e.g., in string escaping),
inlining small helpers, and caching serialized results (reusing stored
JSON string let us skip the full walk/escaping/formatting/dispatching).

In terms of operations in JSON, we may focus on character escaping,
floating-point number formatting, and extra formatting like indentation
and key sorting.
________________________________________





                        (3) Optimizations (3)

After an examination of ceval.c and call.c (The interpreter), We targeted four key hotspots inside _json.c. 
Each was responsible for repeated micro-costs in the flame graph. 
The goal was to reduce per-element work by adding direct fast paths, handling data in larger chunks, and cutting unnecessary branching.
________________________________________
1. py_encode_basestring_ascii 

Role: The “front door” called from Python (_json.encode_bas..). The string-escaping helper, used when encoding Python str objects.
Before: Each character was checked one by one, branching on quotes, control characters, or non-ASCII. 
Even if the string was clean ASCII, it still went through per-char logic.

Optimization:
•	Added a fast path for pure-ASCII strings that need no escaping: copy the entire payload with memcpy and wrap it in quotes.
•	For all other cases, fall back to the standard escaper (per‑char write for safe chars, escaping only when needed).
Effect: Common ASCII‑heavy text takes the fast path; mixed/non‑ASCII still use the robust escaper.
________________________________________
2. encoder_listencode_obj

Role: The central “router” for encoding any Python object to JSON. For example, decides if it’s a str → encode as JSON string.
Before: For every element, the encoder checked multiple possibilities (is it a str, int, float, dict, list, etc.) 
through generic type logic.

Optimization:
•	Added exact-type checks (*_CheckExact) for the common built-ins: str/int/float....
•	Routed these directly to their specialized encoders without going through slower generic checks.
Effect: Reduces call/dispatch overhead and branches per element. Most items in benchmark data hit these fast paths.
________________________________________
3. encoder_listencode_dict                              /* MAIN SPEEDUP */

Role: Convert a Python dict into a JSON object. It’s the backbone for serializing Python dicts
Before: Each key–value pair required repeated checks: key type, options like sorting, escaping decisions, and separator handling.

Optimization:
•	Iterated with PyDict_Next for efficient traversal on the unsorted path (avoids building list(dct.items()) and tuple temporaries). 
    •	"unsorted path" means when JSON output doesn't need alphabetical key ordering. 
    •	PyDict_Next lets us walk through dictionary entries one-by-one directly from Python's internal hash table,
        without first copying all entries into a temporary list. The sorted path keeps the original build+sort semantics to preserve behavior.
•	Reduced per-entry overhead by separating the sorted vs. unsorted path once, and reusing the chosen path inside the loop. 
Effect: Fewer allocations and less refcount churn for unsorted dicts; overall lower per-key overhead.
________________________________________
4. encoder_listencode_list

Role: Convert a Python list/tuple into a JSON array.
Before: For each element, the loop re-checked options and emitted separators through branches. 
Nested structures triggered recursive calls.

Optimization:
•	Flattened the loop: pre-emit separators using a simple counter, no per-item branch for commas.
•	Added simple-type fast paths (None/True/False/str/int/float) that encode directly without extra dispatch.
Effect: Per-item work is smaller for common element types, separators are emitted with minimal branching, and long lists scale better.
________________________________________
Techniques used (across functions)

•	Bulk copy where safe: memcpy from <string.h> for the ASCII no‑escape case in string encoding (py_encode_basestring_ascii)
•	Exact-type fast paths: skip generic logic for common built-ins (str/int/float/etc.).
•	Branch hoisting: choose sorted vs. unsorted dict iteration once; keep the inner loop lean.
•	Inline simple paths: handle common list element types directly; fall back to the generic encoder for the rest.
________________________________________





                    (4) Performance Comparison (4)


Summary
•	End-to-end time improved by ~15% (23.5 → 20.0 ms).
•	JSON C encoder work got cheaper: string-escape and type-dispatch shares dropped.
•	More bytes moved per operation: libc mem* share rose (bulk copies).
•	Interpreter’s relative share increased (same cost divided by a smaller total).
________________________________________

Item                                       |original|Optimized|Δ (abs)| Δ%     | Notes
------------------------------------------+------------+------------+------------+-----------+---------------------
Time mean (ms)                             | 23.500 | 20.000 | -3.500 | -14.9% | comp.txt
IPC                                        | 2.377  | 2.363  | -0.014 | -0.6%  | comp.txt
MPKI                                       | 0.0127 | 0.0116 | ~      | ~      | comp.txt

Overhead%: _PyEval_EvalFrameDefault        | 12.870 | 15.580 | 2.710  | +21.1% | interpreter (relative share)
Overhead%: py_encode_basestring_ascii      | 5.000  | 3.040  | -1.960 | -39.2% | string escape
Overhead%: _PyUnicode_JoinArray            | 4.710  | 5.130  | 0.420  | +8.9%  | string joining
Overhead%: encoder_listencode_obj          | 2.950  | 1.010  | -1.940 | -65.8% | type dispatch
Overhead%: encoder_listencode_dict         |        | 2.360  |        |        | dict encoder (visible after)
Overhead%: __memmove_avx_unaligned_erms_rtm| 1.800  | 2.980  | 1.180  | +65.6% | bulk copy rises
Overhead%: lookdict_unicode_nodummy | 2.940 | 2.720 | -0.220 | -7.5%  |          dict lookup
________________________________________
Insights 
•	Smaller JSON-escape leaf: The share for py_encode_basestring_ascii fell (5.0% → 3.0%). Less per-char branching; more span copies → narrower leaf, faster path.
•	Type-dispatch shrank: encoder_listencode_obj share dropped sharply (≈−66%). Exact-type fast paths reduce dispatch cost per element.
•	Bulk-copy widened: libc memmove/memcpy share increased. This matches “copy safe spans in chunks,” trading control flow for high-throughput copies.
•	Interpreter taller (relative): _PyEval_EvalFrameDefault share rose. Expected sense JSON leafs got faster.
•	Dict encoder surfaced: encoder_listencode_dict became a distinct hot area after optimizations (didn't appear before optimization), indicating the remaining work is in container emission rather than escaping/dispatch.
•	Joining pressure persists: _PyUnicode_JoinArray went up a bit. Bigger chunks mean larger final joins still carry cost.
•	Frame setup/teardown more visible: Prologue/epilogue instructions in _PyEval_EvalFrameDefault carry higher shares post-opt, again a relative effect of cheaper JSON work.
•	String escape lighter: Report shows py_encode_basestring_ascii share down ~39%. Reason: fewer escape checks and more bulk writes per string.
•	Dispatch lighter: encoder_listencode_obj down ~66%. Reason: exact-type paths bypass generic checks.
________________________________________
Remaining Bottlenecks & Next Actions
•	Interpreter dispatch dominates (relative): Reduce round-trips into Python.
•	Dict emission: Optimize per-entry handling (encoder_listencode_dict, PyDict_Next).
•	String joining: Optimize _PyUnicode_JoinArray pressure.
_____________________________________






                    (5) Hardware Acceleration Proposal (5)

                        JSON string Escape Accelerator

Role: Accelerates the hottest text path: scanning strings, copying “safe” runs, and emitting escapes only where needed.
________________________________________
Inputs / Outputs
•	Inputs: We need to know where the bytes start, and how many. Hence, pointer + int length. 
            Provided from the CPU (similarly to GPU)
•	Outputs: A contiguous JSON text + int length
________________________________________
HW/SW interface
•	Placement: In SoC. We can connect it using our beloved PCIe from lectures.
•	Data movement: We will use a cyclic queue. Each slot contains a singular input tuple - jobs (pointer+len) from the CPU.
    This way, the CPU can keep dropping new jobs into free slots, and the accelerator will always find the next one without stopping.
    DMA is crucial for reading input, writing output flow (Unless we want a memory-bound bottleneck).

•	Driver: If CUDA was invented for the GPU, we also need a small and simple library to work on. 
    Comes with an OS driver to match the various hardware environments.
________________________________________
Why it speeds things up:
•	Software: per-character loop does multiple checks (quote/control/non-ASCII), branches often, and writes one char at a time (before our optimizations).
              We can infer throughput  1–2 bytes per iteration, with branch mispredicts and small writes slowing it further.
•	Homemade Accelerator: a tiny pipeline scans 16–32 bytes per cycle, detects “safe runs,” and copies them in bulk. 
                          branches (state changes) only at escape boundaries. Fewer branches, wider copies, better prefetch/DMA.
                          Let's be pessimistic and assume it is *10× faster*.

•	Now let's be optimistic and assume that profiling shows string escaping is about *20%* (in our specific benchmark is around 5%).
•	.Amdahl’s Law: 20% gets 10× faster:
        Overall speedup = 1 / ((1 − p) + p / s) = 1 / (0.80 + 0.20 / 10) = 1.22 = 22%
        For our benchmark: 1 / [(1 − 0.05) + 0.05/10] = 1.047 = 4.7%

•	Conclusion: Before you design expensive homemade accelerator, calculate Amdahl's Law.

________________________________________

1. Cyclic queue (job queue)
2. Input pipeline
3. Character classification unit (Tells if a character is "safe"/"needs escape")
4. Escape unit
5. Bulk Copy Unit
   • if characters are safe, copies whole run at once (like a hardware memcpy)
6. Control logic
7. DMA interface


                 +----------------------+
                 |        CPU           |
                 | (Python + _json.c)   |
                 +----------+-----------+
                            |
                  Job(input addr, length)
                            |
                            v
              +-------------+--------------+
              |   Cyclic Job Queue (RAM)   |
              +-------------+--------------+
                            |
                            v
                 +----------+-----------+
                 |   Accelerator Core   |
                 |  (JSON Escape Unit)  |
                 +----------+-----------+
                            |
        +-------------------+-------------------+
        |                   |                   |
        v                   v                   v
+---------------+   +---------------+   +---------------+
|   Character   |   |     Escape    |   |   Bulk Copy   |
|   Classify    |-->|  (e.g. "→\")  |   |    (safe)     |
+---------------+   +---------------+   +---------------+
        \                   |                   /
         \                  v                  /
          +----------------------------------+
          |   Control Logic + DMA Interface  |
          | (fetch input, write output RAM)  |
          +----------------------------------+
                            |
                      JSON-safe string
                            v
                 +----------+-----------+
                 |        RAM           |
                 +----------------------+

_______________________________________
Performance / Area / Power Trade-offs

•	Performance: Big gains on long ASCII-heavy strings, since the accelerator copies many characters at once.
                 For very short strings, the benefit is smaller (software may even be faster).
•	Area: Needs only small lookup tables, some buffers, and simple copy/escape logic.
          Takes modest space compared to big units like a CPU core.
•	Power: Streaming design is energy-efficient because it avoids branch mispredicts and many small CPU instructions.
           But using DMA and running extra hardware does add some power overhead.
•	Balance: The accelerator is worthwhile when workloads have many large strings.
             For tiny strings or rare JSON use, the cost of extra hardware and power may not be justified.